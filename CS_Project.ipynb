{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS Project.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4oVfZQHL5iqj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "0e41ba53-3180-4c5a-cde1-b57f87b7f7dd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529025191877,
          "user_tz": 240,
          "elapsed": 5665,
          "user": {
            "displayName": "Matthew Stanciu",
            "photoUrl": "//lh6.googleusercontent.com/-5w34I6HOtTA/AAAAAAAAAAI/AAAAAAAAAXM/6tt4xJRRcCU/s50-c-k-no/photo.jpg",
            "userId": "105736452470826651375"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDictionary"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDictionary\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/b3/3014f6ee372f93d7bc1c743b020d0b223d7e792f2aa4f117466991832ba1/PyDictionary-1.5.2.zip\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (4.6.0)\n",
            "Collecting goslate (from PyDictionary)\n",
            "  Downloading https://files.pythonhosted.org/packages/39/0b/50af938a1c3d4f4c595b6a22d37af11ebe666246b05a1a97573e8c8944e5/goslate-1.5.1.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (2.18.4)\n",
            "Collecting click (from PyDictionary)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 4.5MB/s \n",
            "\u001b[?25hCollecting futures (from goslate->PyDictionary)\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/26/b61e3a4eb50653e8a7339d84eeaa46d1e93b92951978873c220ae64d0733/futures-3.1.1.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2018.4.16)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (1.22)\n",
            "Building wheels for collected packages: PyDictionary, goslate, futures\n",
            "  Running setup.py bdist_wheel for PyDictionary ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/8f/55/4a/14dc8470ed285bf2463f958022322e8de7c46c2555c45e3007\n",
            "  Running setup.py bdist_wheel for goslate ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/4f/7f/28/6f52271012a7649b54b1a7adaae329b4246bbbf9d1e4f6e51a\n",
            "  Running setup.py bdist_wheel for futures ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f3/f9/c7/4fbf1faa6038faf183f6e3ea61f17a5f7eea5ab9a1dd7753fd\n",
            "Successfully built PyDictionary goslate futures\n",
            "Installing collected packages: futures, goslate, click, PyDictionary\n",
            "Successfully installed PyDictionary-1.5.2 click-6.7 futures-3.1.1 goslate-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_cuIdbiCoNJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explanation\n",
        "First we need to install TensorFlow in the machine. Then `import` them.\n"
      ]
    },
    {
      "metadata": {
        "id": "T3r1F-MgzB9q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from contextlib import closing\n",
        "import codecs\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path\n",
        "from PyDictionary import PyDictionary\n",
        "pydict = PyDictionary()\n",
        "import urllib.request as urllib\n",
        "\n",
        "# Reproducability :D \n",
        "np.random.seed(7)\n",
        "\n",
        "randomTextURL = 'https://raw.githubusercontent.com/carykh/neuralNetworkLanguageDetection/master/data/output0.txt'\n",
        "keyMashTextURL = 'https://raw.githubusercontent.com/carykh/neuralNetworkLanguageDetection/master/data/output1.txt'\n",
        "#bigNotGibberishURL = 'https://raw.githubusercontent.com/rrenaud/Gibberish-Detector/master/big.txt'\n",
        "\n",
        "randomURLResponse = requests.get(randomTextURL)\n",
        "keyMashURLResponse = requests.get(keyMashTextURL)\n",
        "#bigNotGibberishURLResponse = request.urlopen(bigNotGibberishURL)\n",
        "\n",
        "randomText = csv.reader(randomURLResponse.text)\n",
        "keyMashText = csv.reader(keyMashURLResponse.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--SXcmosrgNJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "95264a36-1f24-4e29-db34-64487c31a79e",
        "executionInfo": {
          "status": "error",
          "timestamp": 1529025216848,
          "user_tz": 240,
          "elapsed": 324,
          "user": {
            "displayName": "Matthew Stanciu",
            "photoUrl": "//lh6.googleusercontent.com/-5w34I6HOtTA/AAAAAAAAAAI/AAAAAAAAAXM/6tt4xJRRcCU/s50-c-k-no/photo.jpg",
            "userId": "105736452470826651375"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_words, data_labels = [], []\n",
        "\n",
        "with requests.get(randomTextURL).text as randomText:\n",
        "  \n",
        "  for row in randomText:\n",
        "    data_words.append(row[0])\n",
        "    data_labels.append('Random Text')\n",
        "  \n",
        "for row in keyMash:\n",
        "  data_words.append(row[0])\n",
        "  data_labels.append('Keyboard Mash')\n",
        "  \n",
        "#for word in bigNotGibberish:\n",
        "#  pass # something\n",
        "  \n",
        "# lets take 80% data as training and remaining 20% for test.\n",
        "train_size = int(len(data) * .8)\n",
        " \n",
        "train_words = data_words[:train_size]\n",
        "train_labels = data_labels[:train_size]\n",
        "\n",
        "test_words = data_words[train_size:]\n",
        "test_labels = data_labels[train_size:]\n",
        "  \n",
        "# 2 different types\n",
        "num_labels = 2\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        " \n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        " \n",
        "x_train = tokenizer.texts_to_matrix(train_words, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_words, mode='tfidf')\n",
        " \n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_labels)\n",
        "y_test = encoder.transform(test_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0b978accfc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomTextURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrandomText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandomText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: __enter__"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "j62FKzP9zcjI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(vocab_size,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43p4UAAQ4yn2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Given a single, one-line string, determine if it contains an english word or\n",
        "# not. If it does, determine whether this random text was produced via keyboard\n",
        "# mashing or if it was randomly generated by a computer. Keyboard mashing likely\n",
        "# contains letters around a certain key: if one key is pressed, as well as 6\n",
        "# surrounding keys (at least 4), and this pattern appears for 75% of all of the\n",
        "# random text, then it was probably mashed.\n",
        "\n",
        "def determine_gibberish(line):\n",
        "  gibberish = True\n",
        "  gibberish_count = 0\n",
        "  total_count = 0\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    if pydict.meaning(word) is None:\n",
        "      gibberish_count+=1\n",
        "      total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "      \n",
        "  if gibberish_count / total_count < 0.25:\n",
        "    gibberish = False\n",
        "    \n",
        "  return gibberish\n",
        "\n",
        "def percent_gibberish(line):\n",
        "  gibberish = True\n",
        "  gibberish_count = 0\n",
        "  total_count = 0\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    if pydict.meaning(word) is None:\n",
        "      gibberish_count+=1\n",
        "      total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "      \n",
        "  return gibberish_count / total_count\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yx0uXBwy_wr0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def determine_mashing(line):\n",
        "  # assumes line is gibberish\n",
        "  # key set only includes the alphabet, not the other keys on the keyboard because i'm lazy\n",
        "  common_keys = {\"Q\":[\"W\",\"A\"],\"W\":[\"Q\",\"A\",\"S\",\"D\",\"E\"], \"E\":[\"W\",\"S\",\"D\",\"F\",\"R\"], \"R\":[\"E\",\"D\",\"F\",\"G\",\"T\"],\"T\":[\"R\",\"F\",\"G\",\"H\",\"Y\"],\n",
        "                \"Y\":[\"T\",\"G\",\"H\",\"J\",\"Y\"], \"U\":[\"Y\",\"H\",\"J\",\"I\",\"K\"], \"I\":[\"U\",\"J\",\"K\",\"L\",\"O\"], \"O\":[\"I\",\"K\",\"L\",\"P\"],\n",
        "                \"A\":[\"Q\",\"W\",\"S\",\"Z\"],\"S\":[\"Q\",\"W\",\"E\",\"D\",\"X\",\"Z\",\"A\"],\"D\":[\"W\",\"E\",\"R\",\"F\",\"C\",\"X\",\"S\"],\"F\":[\"D\",\"E\",\"R\",\"T\",\"G\",\"V\",\"C\"],\n",
        "                \"G\":[\"R\",\"T\",\"Y\",\"H\",\"B\",\"V\",\"F\"],\"H\":[\"T\",\"Y\",\"U\",\"J\",\"N\",\"B\",\"G\"],\"J\":[\"Y\",\"U\",\"I\",\"K\",\"M\",\"N\",\"H\"],\n",
        "                \"K\":[\"U\",\"I\",\"O\",\"L\",\"M\",\"J\"],\"L\":[\"I\",\"O\",\"P\",\"K\",\"M\"],\"Z\":[\"A\",\"S\",\"X\"],\"X\":[\"Z\",\"S\",\"D\",\"C\"],\"C\":[\"X\",\"D\",\"F\",\"V\"],\n",
        "                \"V\":[\"C\",\"F\",\"G\",\"B\"],\"B\":[\"V\",\"G\",\"H\",\"N\"],\"N\":[\"B\",\"H\",\"J\",\"M\"],\"M\":[\"N\",\"J\",\"K\",\"L\",\",\"]}\n",
        "  \n",
        "  mashing = False\n",
        "  mash_count = 0\n",
        "  total_count = 0\n",
        "  words = str(line.split())\n",
        "  letters = list(words)\n",
        "\n",
        "  for word in words: # I think this might not work because it doesn't move on to another word until it checked the same thing a bunch of times\n",
        "    if words.index(word) + 3 is not None and words.index(word) - 3 is not None:\n",
        "      for letter in letters:   \n",
        "        if ((letters.index(letter) in common_keys and (letters[letters.index(letter) + 1] in common_keys\n",
        "                                                    and letters[letters.index(letter) + 2] in common_keys\n",
        "                                                    and letters[letters.index(letter) + 3] in common_keys)) \n",
        "        or (letters.index(letter) in common_keys and (letters[letters.index(letter) - 1] in common_keys\n",
        "                                                     and letters[letters.index(letter) - 2] in common_keys\n",
        "                                                     and letters[letters.index(letter) - 3] in common_keys))):\n",
        "          mash_count+=1\n",
        "          total_count+=1\n",
        "    elif words.index(word) + 2 is not None and words.index(word) - 2 is not None:\n",
        "      for letter in letters:\n",
        "        if ((letters.index(letter) in common_keys and (letters[letters.index(letter) + 1] in common_keys\n",
        "                                                     and letters.index(letter) + 2 in common_keys))\n",
        "        or (letters.index(letter) in common_keys and (letters.index(letter) - 1 in common_keys\n",
        "                                                     and letters.index(letter) - 2 in common_keys))):\n",
        "          mash_count+=1\n",
        "          total_count+=1\n",
        "    elif words.index(word) + 1 is not None and words.index(word) - 1 is not None:\n",
        "      for letter in letters:\n",
        "        if ((letters.index(letter) in common_keys and (letters.index(letter) + 1 in common_keys))\n",
        "        or (letters.index(letter) in common_keys and (letters.index(letter) - 1 in common_keys))):\n",
        "          mash_count+=1\n",
        "          total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "      \n",
        "  if (mash_count/total_count > 0.75):\n",
        "    mashing = True\n",
        "    \n",
        "  return mashing\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJlm0NfyyUig",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "c23337ed-8d6d-4c2c-9d8f-9dee8d802a07",
        "executionInfo": {
          "status": "error",
          "timestamp": 1529025634809,
          "user_tz": 240,
          "elapsed": 3303,
          "user": {
            "displayName": "Matthew Stanciu",
            "photoUrl": "//lh6.googleusercontent.com/-5w34I6HOtTA/AAAAAAAAAAI/AAAAAAAAAXM/6tt4xJRRcCU/s50-c-k-no/photo.jpg",
            "userId": "105736452470826651375"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Prompt the user for input for a file, examine the file line by line, and determine\n",
        "# 1) whether or not it is gibberish, as well as the percent gibberish,\n",
        "# and 2) if it is gibberish, the percent mashed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  gibberish_count = 0\n",
        "  total_count = 0\n",
        "  mashing_count = 0\n",
        "  total_mashing_count = 0\n",
        "  \n",
        "  file = input(\"Enter the full link to the file you wish to examine: \")\n",
        "  data = urllib.urlopen(file)\n",
        "  for line in data:\n",
        "    if determine_gibberish(line):\n",
        "        gibberish_count+=1\n",
        "        total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "\n",
        "    if determine_mashing(line):\n",
        "      mashing_count+=1\n",
        "      total_mashing_count+=1\n",
        "    else:\n",
        "      total_mashing_count+=1\n",
        "        \n",
        "  calculated_gibberish = gibberish_count / total_count\n",
        "  calculated_mashing = mashing_count / total_mashing_count\n",
        "  print(\"Your file is \" + calculated_gibberish*100 + \"% gibberish\")\n",
        "    \n",
        "  if calculated_gibberish >= 0.75 and calculated_mashing >= 0.75:\n",
        "    print(\"Your file is determined to be gibberish and the result of keyboard mashing\")\n",
        "  elif calculated_gibberish >= 0.75 and calculated_mashing < 0.75:\n",
        "    print(\"Your file is determined to be gibberish and the result of some other form of random generation -- not keyboard mashing\")\n",
        "  else:\n",
        "    print(\"Your file is determined to not be gibberish\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the full link to the file you wish to examine: https://raw.githubusercontent.com/carykh/neuralNetworkLanguageDetection/master/data/output1.txt\n",
            "Error: The Following Error occured: list index out of range\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 193 of the file /usr/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
            "\n",
            " BeautifulSoup(YOUR_MARKUP})\n",
            "\n",
            "to this:\n",
            "\n",
            " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
            "\n",
            "  markup_type=markup_type))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-25b0ee4d1729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mtotal_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdetermine_mashing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mmashing_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mtotal_mashing_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-98048cec1b1d>\u001b[0m in \u001b[0;36mdetermine_mashing\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mtotal_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmash_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mmashing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}