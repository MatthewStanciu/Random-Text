{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4oVfZQHL5iqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "b93730eb-8249-4fe9-cdef-1a750f0e8f7e"
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDictionary"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDictionary in /usr/local/lib/python3.6/dist-packages (1.5.2)\r\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (1.5.1)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (2.18.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (4.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from PyDictionary) (6.7)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.6/dist-packages (from goslate->PyDictionary) (3.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary) (2018.4.16)\n",
            "usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]\n",
            "           [-D [bind_address:]port] [-E log_file] [-e escape_char]\n",
            "           [-F configfile] [-I pkcs11] [-i identity_file]\n",
            "           [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec]\n",
            "           [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address]\n",
            "           [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]\n",
            "           [user@]hostname [command]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_cuIdbiCoNJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explanation\n",
        "First we need to install TensorFlow in the machine. Then `import` them.\n"
      ]
    },
    {
      "metadata": {
        "id": "T3r1F-MgzB9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from contextlib import closing\n",
        "import codecs\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path\n",
        "from PyDictionary import PyDictionary\n",
        "pydict = PyDictionary()\n",
        "\n",
        "# Reproducability :D \n",
        "np.random.seed(7)\n",
        "\n",
        "randomTextURL = 'https://raw.githubusercontent.com/carykh/neuralNetworkLanguageDetection/master/data/output0.txt'\n",
        "keyMashTextURL = 'https://raw.githubusercontent.com/carykh/neuralNetworkLanguageDetection/master/data/output1.txt'\n",
        "#bigNotGibberishURL = 'https://raw.githubusercontent.com/rrenaud/Gibberish-Detector/master/big.txt'\n",
        "\n",
        "randomURLResponse = requests.get(randomTextURL)\n",
        "keyMashURLResponse = requests.get(keyMashTextURL)\n",
        "#bigNotGibberishURLResponse = request.urlopen(bigNotGibberishURL)\n",
        "\n",
        "randomText = csv.reader(randomURLResponse.text)\n",
        "keyMashText = csv.reader(keyMashURLResponse.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--SXcmosrgNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "8c3816ef-fa63-448d-8886-bba995fbe9f4"
      },
      "cell_type": "code",
      "source": [
        "data_words, data_labels = [], []\n",
        "\n",
        "for row in randomText:\n",
        "  data_words.append(row[0])\n",
        "  data_labels.append('Random Text')\n",
        "  \n",
        "for row in keyMash:\n",
        "  data_words.append(row[0])\n",
        "  data_labels.append('Keyboard Mash')\n",
        "  \n",
        "#for word in bigNotGibberish:\n",
        "#  pass # something\n",
        "  \n",
        "# lets take 80% data as training and remaining 20% for test.\n",
        "train_size = int(len(data) * .8)\n",
        " \n",
        "train_words = data_words[:train_size]\n",
        "train_labels = data_labels[:train_size]\n",
        "\n",
        "test_words = data_words[train_size:]\n",
        "test_labels = data_labels[train_size:]\n",
        "  \n",
        "# 2 different types\n",
        "num_labels = 2\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        " \n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        " \n",
        "x_train = tokenizer.texts_to_matrix(train_words, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_words, mode='tfidf')\n",
        " \n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_labels)\n",
        "y_test = encoder.transform(test_labels)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['R']\n",
            "['G']\n",
            "['P']\n",
            "['W']\n",
            "['', '']\n",
            "['1']\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ea0dfb93053a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandomText\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdata_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mdata_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Random Text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "j62FKzP9zcjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(vocab_size,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43p4UAAQ4yn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Given a single, one-line string, determine if it contains an english word or\n",
        "# not. If it does, determine whether this random text was produced via keyboard\n",
        "# mashing or if it was randomly generated by a computer. Keyboard mashing likely\n",
        "# contains letters around a certain key: if one key is pressed, as well as 6\n",
        "# surrounding keys (at least 4), and this pattern appears for 75% of all of the\n",
        "# random text, then it was probably mashed.\n",
        "\n",
        "def determine_gibberish(line):\n",
        "  gibberish = True\n",
        "  gibberish_count = 0\n",
        "  total_count = 0\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    if pydict.meaning(word) is None:\n",
        "      gibberish_count+=1\n",
        "      total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "      \n",
        "  if gibberish_count / total_count < 0.25:\n",
        "    gibberish = False\n",
        "    \n",
        "  return gibberish\n",
        "\n",
        "def percent_gibberish(line):\n",
        "  gibberish = True\n",
        "  gibberish_count = 0\n",
        "  total_count = 0\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    if pydict.meaning(word) is None:\n",
        "      gibberish_count+=1\n",
        "      total_count+=1\n",
        "    else:\n",
        "      total_count+=1\n",
        "      \n",
        "  return gibberish_count / total_count\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yx0uXBwy_wr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def determine_mashing(list):\n",
        "  # assumes list is gibberish\n",
        "  # key set only includes the alphabet, not the other keys on the keyboard because i'm lazy\n",
        "  common_keys = {\"Q\":[\"W\",\"A\"],\"W\":[\"Q\",\"A\",\"S\",\"D\",\"E\"], \"E\":[\"W\",\"S\",\"D\",\"F\",\"R\"], \"R\":[\"E\",\"D\",\"F\",\"G\",\"T\"],\"T\":[\"R\",\"F\",\"G\",\"H\",\"Y\"],\n",
        "                \"Y\":[\"T\",\"G\",\"H\",\"J\",\"Y\"], \"U\":[\"Y\",\"H\",\"J\",\"I\",\"K\"], \"I\":[\"U\",\"J\",\"K\",\"L\",\"O\"], \"O\":[\"I\",\"K\",\"L\",\"P\"],\n",
        "                \"A\":[\"Q\",\"W\",\"S\",\"Z\"],\"S\":[\"Q\",\"W\",\"E\",\"D\",\"X\",\"Z\",\"A\"],\"D\":[\"W\",\"E\",\"R\",\"F\",\"C\",\"X\",\"S\"],\"F\":[\"D\",\"E\",\"R\",\"T\",\"G\",\"V\",\"C\"],\n",
        "                \"G\":[\"R\",\"T\",\"Y\",\"H\",\"B\",\"V\",\"F\"],\"H\":[\"T\",\"Y\",\"U\",\"J\",\"N\",\"B\",\"G\"],\"J\":[\"Y\",\"U\",\"I\",\"K\",\"M\",\"N\",\"H\"],\n",
        "                \"K\":[\"U\",\"I\",\"O\",\"L\",\"M\",\"J\"],\"L\":[\"I\",\"O\",\"P\",\"K\",\"M\"],\"Z\":[\"A\",\"S\",\"X\"],\"X\":[\"Z\",\"S\",\"D\",\"C\"],\"C\":[\"X\",\"D\",\"F\",\"V\"],\n",
        "                \"V\":[\"C\",\"F\",\"G\",\"B\"],\"B\":[\"V\",\"G\",\"H\",\"N\"],\"N\":[\"B\",\"H\",\"J\",\"M\"],\"M\":[\"N\",\"J\",\"K\",\"L\",\",\"]}\n",
        "  \n",
        "  mashing = False\n",
        "  mash_count = 0\n",
        "  words = line.split()\n",
        "  # see moleskine notebook for the rest\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}